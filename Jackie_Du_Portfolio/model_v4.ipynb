{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be75cb53-2c24-4b2a-87ad-96c2d8733411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as imbPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2047c-99ce-4585-99c8-77da1257989d",
   "metadata": {},
   "source": [
    "### Prepare data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9de95379-a1a1-40ed-a511-09afdd916c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data / process data \n",
    "df = pd.read_csv('processed-data-02-07.csv', index_col =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b92d98f4-226a-40d4-bbf0-d85af081dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out anything with wind_10m, date\n",
    "df = df.loc[:, df.columns[~df.columns.astype('str').str.contains('wind_10m')]]\n",
    "df = df.drop(columns=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9778232f-2405-40ea-aeab-2e64f38cb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy().drop(columns=[\"is_heat\"])\n",
    "y = df[\"is_heat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a49a0e-634a-44c3-bb24-6daa6d23599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2, random_state=512,\n",
    "                                                    stratify = y\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5fd50-efe1-43a2-8c0e-094d2dd17346",
   "metadata": {},
   "source": [
    "### Select subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f1f6e842-4583-42ed-acfe-55bc1d9de9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a small subset of features\n",
    "\n",
    "# 36 subsets\n",
    "# hi, air temp, wbgt\n",
    "\n",
    "# only day of \n",
    "# 1-day, 3-day \n",
    "# 3 day, 5 day \n",
    "\n",
    "# just mean \n",
    "# mean, max, var\n",
    "# mean, max\n",
    "# mean, var \n",
    "\n",
    "# list of lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bb374d01-ff21-4956-b120-2ca352418b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets= []\n",
    "cols = X_train.columns\n",
    "cols = cols[~cols.str.contains('min')]\n",
    "\n",
    "# first subset by mean / max /var\n",
    "mean_c = cols[cols.str.contains('mean')]\n",
    "mean_max_c = cols[cols.str.contains('mean|max')]\n",
    "mean_var_c = cols[cols.str.contains('mean|var')]\n",
    "\n",
    "# for each of cols, mean_c, mean_max_c, mean_var_c\n",
    "for cols_i in [cols,mean_c, mean_max_c, mean_var_c]:\n",
    "    # take out 3 or 5 lag \n",
    "    a = cols_i[~cols_i.str.contains('lag')]\n",
    "    # take out 3 lag \n",
    "    b = cols_i[~cols_i.str.contains('lag3')]\n",
    "    # take out 5 lag \n",
    "    c = cols_i[~cols_i.str.contains('la5')]\n",
    "    for cols_j in [a, b, c]:\n",
    "        # take out air temp, wbgt\n",
    "        subsets.append(cols_j[~cols_j.str.contains('air_temp|wbgt')])\n",
    "        # take out hi, wbgt\n",
    "        subsets.append(cols_j[~cols_j.str.contains('hi|wbgt')])\n",
    "        # take out hi, air temp\n",
    "        subsets.append(cols_j[~cols_j.str.contains('hi|air_temp')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cc6614-ec7e-4ed6-aa6d-b58be5a4b4e7",
   "metadata": {},
   "source": [
    "### Cross-validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f15abba-587c-421a-98e0-4052b6468070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of these subsets \n",
    "# run gridsearch cv on the models \n",
    "# for each model / resampling method option, calculate cross val score on best params \n",
    "\n",
    "# for the subset, store the avg F1 for each combination of resampling and model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e20101-1724-4585-89d4-de39be00cc21",
   "metadata": {},
   "source": [
    "#### Function for storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a0c945f-6ec5-4eac-b81f-bbe111d5a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_best_res(res, model, resampling_type, subset_ind, neighbors=None):\n",
    "    '''takes a grid search cv fit, calculates 5 fold cross val scores based \n",
    "    on best params fit and stores into cv_res \n",
    "    which feature subset, model, resampling, the resampling params (if app),\n",
    "    the model's best params, avg cross val F1, std cross val F1'''\n",
    "    df = pd.DataFrame(res.cv_results_)\n",
    "    \n",
    "    # cross val score\n",
    "    scores = cross_val_score(both3_gscv, X_train.iloc[:, 0:5], y_train, scoring='f1_weighted')\n",
    "    \n",
    "    \n",
    "    cv_res_i = pd.DataFrame(np.array([[subset_ind, model, resampling_type, neighbors, res.best_params_, scores.mean(), scores.std()]]),\n",
    "                            columns=[\"subset\", \"model\", \"resampling_method\", \"neighbors\", \"params\", \"mean_cv_F1_score\", \"std_cv_F1_score\"])\n",
    "    \n",
    "    return cv_res.append(cv_res_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4ef6c-ea23-4813-a75b-d06dfe720571",
   "metadata": {},
   "source": [
    "#### Initializing things that don't need to be in for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "855b88a1-47ff-4426-9452-43459b73af88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list to store results\n",
    "cv_res = []\n",
    "\n",
    "# initialize stratified k fold \n",
    "folds = StratifiedKFold(n_splits=5, random_state=512, shuffle=True)\n",
    "\n",
    "# no resampling \n",
    "none_en_pipe = imbPipeline([\n",
    "    ('classifier', LogisticRegression()) \n",
    "])\n",
    "\n",
    "none_xg_pipe = imbPipeline([\n",
    "    ('classifier', XGBClassifier()) \n",
    "])\n",
    "\n",
    "# set params for steps of pipeline \n",
    "xg_param_grid = [\n",
    "    {'classifier' : [XGBClassifier()],\n",
    "    'classifier__n_estimators' : list(range(25,251,2)),\n",
    "     'classifier__max_depth' : list(range(1,6)),\n",
    "     'classifier__eta' : list(np.arange(0.1,0.7,0.2))\n",
    "    }]\n",
    "\n",
    "en_param_grid = [\n",
    "    {'classifier' : [LogisticRegression(penalty='elasticnet', solver='saga', tol = 0.01)],\n",
    "    'classifier__l1_ratio' : list(np.arange(0.1, 1., 0.4)),\n",
    "    'classifier__C': list(np.arange(0.1, 1., 0.4))}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e006cdbd-bee7-4ab0-adb9-587340f646f1",
   "metadata": {},
   "source": [
    "#### for loop of training!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9048bb00-1cab-4c58-80bb-61649a0e95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, features_j in enumerate(subsets):\n",
    "    # subset X for features j \n",
    "    X_train_j = X_train[features_j]\n",
    "    X_val_j = X_val[features_j]\n",
    "    \n",
    "    ## NO RESAMPLING \n",
    "    # grid search \n",
    "    none_en_gscv = GridSearchCV(none_en_pipe, param_grid = en_param_grid, \n",
    "          cv = folds, verbose=0, scoring=\"f1_weighted\")\n",
    "    none_xg_gscv = GridSearchCV(none_xg_pipe, param_grid = xg_param_grid, \n",
    "          cv = folds, verbose=0, scoring=\"f1_weighted\")\n",
    "    # fit \n",
    "    none_en_gscv.fit(X_train_j, y_train_j)\n",
    "    none_xg_gscv.fit(X_train_j, y_train_j)\n",
    "    # store res \n",
    "    store_best_res(none_en_gscv, 'elastic_net', 'none', i)\n",
    "    store_best_res(none_xg_gscv, 'xgboost', 'none', i)\n",
    "    \n",
    "    \n",
    "    ## RESAMPLING \n",
    "    for smote_n in [3,5,7]:\n",
    "        # smote en\n",
    "        smote_en_pipe = imbPipeline([\n",
    "            ('over', SMOTE(sampling_strategy = 0.55, k_neighbors= smote_n)),\n",
    "            ('classifier', LogisticRegression()) \n",
    "        ])\n",
    "        # smote xg\n",
    "        smote_xg_pipe = imbPipeline([\n",
    "            ('over', SMOTE(sampling_strategy = 0.55, k_neighbors= smote_n)),\n",
    "            ('classifier', XGBClassifier()) \n",
    "        ])\n",
    "        # smote and undersampling en\n",
    "        both_en_pipe = imbPipeline([\n",
    "            ('over', SMOTE(sampling_strategy = 0.55, k_neighbors= smote_n)), \n",
    "            ('under', RandomUnderSampler()),\n",
    "            ('classifier', LogisticRegression()) \n",
    "        ])\n",
    "        # smote and undersampling xg\n",
    "        both_xg_pipe = imbPipeline([\n",
    "            ('over', SMOTE(sampling_strategy = 0.55, k_neighbors= smote_n)), \n",
    "            ('under', RandomUnderSampler()),\n",
    "            ('classifier', XGBClassifier()) \n",
    "        ])\n",
    "\n",
    "        # grid search  \n",
    "        smote_en_gscv = GridSearchCV(smote_en_pipe, param_grid = en_param_grid, \n",
    "                                 cv = folds, verbose=0, scoring=\"f1_weighted\")\n",
    "        smote_xg_gscv = GridSearchCV(smote_xg_pipe, param_grid = xg_param_grid, \n",
    "                                 cv = folds, verbose=0, scoring=\"f1_weighted\")\n",
    "        both__en_gscv = GridSearchCV(both_en_pipe, param_grid = en_param_grid, \n",
    "                                  cv = folds, verbose=0, scoring=\"f1_weighted\")\n",
    "        both__xg_gscv = GridSearchCV(both_xg_pipe, param_grid = xg_param_grid, \n",
    "                                  cv = folds, verbose=0, scoring=\"f1_weighted\")\n",
    "        \n",
    "        # fit \n",
    "        smote_en_gscv.fit(X_train_j, y_train_j)\n",
    "        smote_xg_gscv.fit(X_train_j, y_train_j)\n",
    "        both_en_gscv.fit(X_train_j, y_train_j)\n",
    "        both_xg_gscv.fit(X_train_j, y_train_j)\n",
    "        \n",
    "        store_best_res(smote_en_gscv, 'elastic_net', 'smote', i, smote_n)\n",
    "        store_best_res(smote_xg_gscv, 'xgboost', 'smote', i, smote_n)\n",
    "        store_best_res(both_en_gscv, 'elastic_net', 'both', i, smote_n)\n",
    "        store_best_res(both_xg_gscv, 'xgboost', 'both', i, smote_n)\n",
    "    \n",
    "    print(\"Done with subset \", i, \"/\", len(subsets))\n",
    "\n",
    "# after everything done, have list of dataframes \n",
    "final_cv_res_df= pd.concat(cv_res)\n",
    "\n",
    "# write to pickle\n",
    "final_cv_res_df.to_pickle('cv_res.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
